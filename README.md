# SynÂ³ (Synopsis Cubed)

**A Three-Dimensional Neural Architecture for Multi-Document Summarization**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Overview

SynÂ³ introduces a novel neural architecture that operates across three dimensions of synthesis:

- **SynÂ¹**: Document synthesis through advanced encoding
- **SynÂ²**: Cross-document synergy via attention mechanisms  
- **SynÂ³**: Synopsis generation through hierarchical decoding

Unlike traditional summarization models that process documents independently, SynÂ³ learns rich relationships between multiple documents to generate coherent, comprehensive summaries.

## ğŸ—ï¸ Architecture

```
Input Documents â†’ Document Encoder (SynÂ¹) â†’ Cross-Document Attention (SynÂ²) â†’ Summary Generator (SynÂ³) â†’ Output Summary
```

### Key Components

1. **Document Encoder**: BERT-based individual document understanding
2. **Cross-Document Attention**: Novel multi-head attention mechanism for document relationships
3. **Hierarchical Decoder**: LSTM-based summary generation with document fusion

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/username/syn3-summarization.git
cd syn3-summarization
pip install -r requirements.txt
```

### Basic Usage

```python
from syn3_model import SynCubed
from datasets import load_dataset

# Load model
model = SynCubed()

# Load Multi-News dataset
dataset = load_dataset("multi_news")

# Train model
python train.py --config baseline --epochs 3

# Evaluate
python evaluate.py --model_path checkpoints/syn3_best.pth
```

## ğŸ“Š Results

| Model | ROUGE-1 | ROUGE-2 | ROUGE-L |
|-------|---------|---------|---------|
| Baseline (Single-doc) | 0.42 | 0.18 | 0.38 |
| **SynÂ³** | **0.48** | **0.23** | **0.44** |

## ğŸ”¬ Experiments

SynÂ³ supports multiple architectural configurations:

- **Attention Variants**: 4, 8, 12 heads
- **Fusion Methods**: Mean, Max, Attention-based
- **Encoder Models**: DistilBERT, BERT, RoBERTa

```bash
# Run different configurations
python train.py --config attention_focused    # 12 heads + connection loss
python train.py --config lightweight         # Fast training variant
python train.py --config roberta_large       # High-performance variant
```

## ğŸ“ˆ Visualization

SynÂ³ provides rich visualizations of document relationships:

- **Attention Heatmaps**: Cross-document relationship strength
- **Connection Networks**: Graph visualization of document clusters
- **Training Curves**: Performance monitoring during training

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch 2.0+
- Transformers 4.0+
- Datasets
- Rouge-score
- NetworkX
- Matplotlib

## ğŸ“ Academic Use

This implementation was developed for ICT303 Assignment 2 at Murdoch University. If you use this code in your research, please cite:

```bibtex
@misc{syn3_2025,
  title={SynÂ³: A Three-Dimensional Neural Architecture for Multi-Document Summarization},
  author={Maveron Tyriel V. Aguares},
  year={2025},
  note={ICT303 Assignment 2, Murdoch University}
}
```

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- [ ] Beam search decoding
- [ ] Copy mechanism integration  
- [ ] Larger model variants
- [ ] Additional evaluation metrics
- [ ] Real-time inference optimization

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **Dataset**: [Multi-News](https://huggingface.co/datasets/multi_news)
- **Paper**: [Coming Soon]
- **Demo**: [Coming Soon]

---

**Built with â¤ï¸ for advancing multi-document understanding**
